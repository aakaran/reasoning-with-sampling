{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Soft Thinking Comparison: Standard vs Vanilla vs Enhanced\n",
    "\n",
    "This notebook provides a comprehensive comparison of multiple sampling strategies:\n",
    "\n",
    "1. **Standard Sampling**: Traditional discrete token generation\n",
    "2. **Vanilla Soft Thinking**: Basic soft token approach\n",
    "3. **Soft Thinking + Dirichlet Noise**: With stochastic perturbation\n",
    "4. **Soft Thinking + Gumbel-Softmax**: With Gumbel noise injection\n",
    "\n",
    "## Enhanced Features from RicardoLaMo/Soft-Thinking-rl\n",
    "\n",
    "### 1. **Noise Injection Strategies**\n",
    "\n",
    "**Dirichlet Noise**:\n",
    "- Samples from Dirichlet distribution: `Dir(α + (1-α)·p)`\n",
    "- Maintains general distribution shape while adding stochasticity\n",
    "- Good for exploring nearby probability regions\n",
    "\n",
    "**Gumbel-Softmax**:\n",
    "- Adds Gumbel noise to logits: `logits' = (logits + Gumbel) / τ`\n",
    "- Reparameterization trick for categorical sampling\n",
    "- Temperature `τ` controls exploration vs exploitation\n",
    "\n",
    "### 2. **Dual Temperature Control**\n",
    "\n",
    "- **Thinking Temperature**: Used during soft thinking steps (exploration)\n",
    "- **Output Temperature**: Used for final token selection (exploitation)\n",
    "- Enables separate control of exploration during reasoning vs final decision\n",
    "\n",
    "### 3. **Multi-Criteria Early Stopping**\n",
    "\n",
    "- **Entropy-based**: Stop when model is confident (low entropy)\n",
    "- **Length-based**: Stop after max thinking steps to control compute\n",
    "- Tracks which criterion triggered stopping for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# Add project path\n",
    "project_root = Path('/home/wliu23/github/reasoning-with-sampling/llm_experiments')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from grader_utils.parse_utils import parse_answer\n",
    "from constants import *\n",
    "from power_samp_utils import AutoregressiveSampler, SoftThinkingSampler, soft_thinking_generate, format_prompt\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up HuggingFace cache\n",
    "HF_HOME = Path(\"/home/wliu23/projects/reasoning-with-sampling/.hf_home\")\n",
    "os.environ[\"HF_HOME\"] = str(HF_HOME)\n",
    "os.environ[\"HF_HUB_CACHE\"] = str(HF_HOME / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_HOME / \"hub\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(HF_HOME / \"datasets\")\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model': 'qwen_math',\n",
    "    'model_str': 'Qwen/Qwen2.5-Math-7B',\n",
    "    'cache_dir': os.environ['TRANSFORMERS_CACHE'],\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Dataset settings\n",
    "    'dataset': 'MATH',\n",
    "    'dataset_path': '/home/wliu23/github/reasoning-with-sampling/llm_experiments/data/MATH500.json',\n",
    "    'num_questions': 20,  # Start small\n",
    "    \n",
    "    # Generation settings\n",
    "    'max_new_tokens': 3072,\n",
    "    'cot': True,\n",
    "    \n",
    "    # Standard sampling\n",
    "    'standard': {\n",
    "        'temperature': 1.0,\n",
    "    },\n",
    "    \n",
    "    # Soft Thinking: Vanilla (no noise)\n",
    "    'soft_vanilla': {\n",
    "        'num_thinking_steps': 3,\n",
    "        'max_topk': 10,\n",
    "        'min_p': 0.001,\n",
    "        'early_stopping_entropy_threshold': 0.05,\n",
    "        'early_stopping_length_threshold': 5,  # Max thinking steps\n",
    "        'thinking_temperature': 1.0,\n",
    "        'output_temperature': 0.7,  # Sharper for final output\n",
    "        'noise_type': 'none',\n",
    "    },\n",
    "    \n",
    "    # Soft Thinking: Dirichlet Noise\n",
    "    'soft_dirichlet': {\n",
    "        'num_thinking_steps': 3,\n",
    "        'max_topk': 10,\n",
    "        'min_p': 0.001,\n",
    "        'early_stopping_entropy_threshold': 0.05,\n",
    "        'early_stopping_length_threshold': 5,\n",
    "        'thinking_temperature': 1.0,\n",
    "        'output_temperature': 0.7,\n",
    "        'noise_type': 'dirichlet',\n",
    "        'noise_alpha': 0.1,  # Concentration parameter\n",
    "    },\n",
    "    \n",
    "    # Soft Thinking: Gumbel-Softmax\n",
    "    'soft_gumbel': {\n",
    "        'num_thinking_steps': 3,\n",
    "        'max_topk': 10,\n",
    "        'min_p': 0.001,\n",
    "        'early_stopping_entropy_threshold': 0.05,\n",
    "        'early_stopping_length_threshold': 5,\n",
    "        'thinking_temperature': 1.0,\n",
    "        'output_temperature': 0.7,\n",
    "        'noise_type': 'gumbel',\n",
    "        'gumbel_tau': 1.0,  # Gumbel-Softmax temperature\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    'save_dir': '/home/wliu23/github/reasoning-with-sampling/notebooks/results/comprehensive_soft_thinking',\n",
    "    'alpha': 0.05,  # Statistical significance\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"  Model: {CONFIG['model_str']}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "print(f\"  Questions: {CONFIG['num_questions']}\")\n",
    "print(f\"  Methods: Standard, Vanilla Soft, Dirichlet Soft, Gumbel Soft\")\n",
    "print(f\"  Output: {CONFIG['save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Initialize Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(CONFIG['dataset_path'], 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} problems from MATH500\")\n",
    "print(f\"Will process {CONFIG['num_questions']} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading model: {CONFIG['model_str']}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model_str'],\n",
    "    cache_dir=CONFIG['cache_dir'],\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_str'],\n",
    "    cache_dir=CONFIG['cache_dir'],\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={'': CONFIG['device']},\n",
    "    trust_remote_code=True,\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "print(f\"✓ Model loaded on {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all samplers\n",
    "print(\"Initializing samplers...\")\n",
    "\n",
    "# 1. Vanilla Soft Thinking\n",
    "soft_vanilla_sampler = SoftThinkingSampler(\n",
    "    model, tokenizer, CONFIG['device'],\n",
    "    **{k: v for k, v in CONFIG['soft_vanilla'].items() if k != 'num_thinking_steps'}\n",
    ")\n",
    "print(\"  ✓ Vanilla Soft Thinking (no noise)\")\n",
    "\n",
    "# 2. Dirichlet Soft Thinking\n",
    "soft_dirichlet_sampler = SoftThinkingSampler(\n",
    "    model, tokenizer, CONFIG['device'],\n",
    "    **{k: v for k, v in CONFIG['soft_dirichlet'].items() if k != 'num_thinking_steps'}\n",
    ")\n",
    "print(\"  ✓ Soft Thinking + Dirichlet Noise\")\n",
    "\n",
    "# 3. Gumbel Soft Thinking\n",
    "soft_gumbel_sampler = SoftThinkingSampler(\n",
    "    model, tokenizer, CONFIG['device'],\n",
    "    **{k: v for k, v in CONFIG['soft_gumbel'].items() if k != 'num_thinking_steps'}\n",
    ")\n",
    "print(\"  ✓ Soft Thinking + Gumbel-Softmax\")\n",
    "\n",
    "print(\"\\nAll samplers initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_standard(model, tokenizer, input_ids, device, max_new_tokens=3072, temperature=1.0) -> Dict:\n",
    "    \"\"\"Standard sampling generation.\"\"\"\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    generated_ids = output.sequences[0][len(input_ids[0]):]\n",
    "    completion = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    \n",
    "    log_probs = []\n",
    "    tokens = []\n",
    "    \n",
    "    for i, token_id in enumerate(generated_ids):\n",
    "        if i < len(output.scores):\n",
    "            logits = output.scores[i][0]\n",
    "            log_prob_dist = F.log_softmax(logits, dim=-1)\n",
    "            token_log_prob = log_prob_dist[token_id].item()\n",
    "            log_probs.append(token_log_prob)\n",
    "        tokens.append(tokenizer.decode([token_id]))\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids.cpu().tolist(),\n",
    "        'log_probs': log_probs,\n",
    "        'cumulative_log_prob': sum(log_probs) if log_probs else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs) if log_probs else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_soft_thinking(soft_sampler, prefix, max_new_tokens, num_thinking_steps, config_name) -> Dict:\n",
    "    \"\"\"Soft Thinking generation with detailed tracking.\"\"\"\n",
    "    token_ids, log_probs, soft_info, avg_thinking_steps = soft_thinking_generate(\n",
    "        soft_sampler, prefix, max_new_tokens, num_thinking_steps\n",
    "    )\n",
    "    \n",
    "    generated_ids = token_ids[len(prefix):]\n",
    "    completion = soft_sampler.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    tokens = [soft_sampler.tokenizer.decode([tid]) for tid in generated_ids]\n",
    "    \n",
    "    # Analyze stopping criteria\n",
    "    stopping_reasons = {'entropy': 0, 'length': 0, 'max_steps': 0}\n",
    "    for info in soft_info:\n",
    "        stopping_reasons[info['stopped_by']] += 1\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids,\n",
    "        'log_probs': log_probs,\n",
    "        'cumulative_log_prob': sum(log_probs) if log_probs else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs) if log_probs else 0.0,\n",
    "        'avg_thinking_steps': avg_thinking_steps,\n",
    "        'soft_info': soft_info,\n",
    "        'total_thinking_steps': sum(info['thinking_steps'] for info in soft_info),\n",
    "        'stopping_reasons': stopping_reasons,\n",
    "        'config': config_name,\n",
    "    }\n",
    "\n",
    "print(\"✓ Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection\n",
    "results = []\n",
    "questions_to_process = dataset[:CONFIG['num_questions']]\n",
    "\n",
    "for idx, data in enumerate(tqdm(questions_to_process, desc=\"Processing MATH problems\")):\n",
    "    question = data['prompt']\n",
    "    correct_answer = data['answer']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {idx+1}/{len(questions_to_process)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Q: {question[:100]}...\")\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = format_prompt(question, CONFIG['model'], tokenizer, CONFIG['cot'])\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(CONFIG['device'])\n",
    "    prefix = [idx.item() for idx in input_ids[0]]\n",
    "    \n",
    "    # 1. Standard sampling\n",
    "    print(\"\\n[Standard Sampling]\")\n",
    "    std_result = generate_standard(\n",
    "        model, tokenizer, input_ids, CONFIG['device'],\n",
    "        CONFIG['max_new_tokens'], CONFIG['standard']['temperature']\n",
    "    )\n",
    "    print(f\"  Answer: {std_result['answer']}\")\n",
    "    print(f\"  Tokens: {std_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {std_result['cumulative_log_prob']:.4f}\")\n",
    "    \n",
    "    # 2. Vanilla Soft Thinking\n",
    "    print(\"\\n[Vanilla Soft Thinking]\")\n",
    "    vanilla_result = generate_soft_thinking(\n",
    "        soft_vanilla_sampler, prefix,\n",
    "        CONFIG['max_new_tokens'],\n",
    "        CONFIG['soft_vanilla']['num_thinking_steps'],\n",
    "        'vanilla'\n",
    "    )\n",
    "    print(f\"  Answer: {vanilla_result['answer']}\")\n",
    "    print(f\"  Tokens: {vanilla_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {vanilla_result['cumulative_log_prob']:.4f}\")\n",
    "    print(f\"  Avg thinking steps: {vanilla_result['avg_thinking_steps']:.2f}\")\n",
    "    \n",
    "    # 3. Dirichlet Soft Thinking\n",
    "    print(\"\\n[Soft Thinking + Dirichlet]\")\n",
    "    dirichlet_result = generate_soft_thinking(\n",
    "        soft_dirichlet_sampler, prefix,\n",
    "        CONFIG['max_new_tokens'],\n",
    "        CONFIG['soft_dirichlet']['num_thinking_steps'],\n",
    "        'dirichlet'\n",
    "    )\n",
    "    print(f\"  Answer: {dirichlet_result['answer']}\")\n",
    "    print(f\"  Tokens: {dirichlet_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {dirichlet_result['cumulative_log_prob']:.4f}\")\n",
    "    print(f\"  Avg thinking steps: {dirichlet_result['avg_thinking_steps']:.2f}\")\n",
    "    \n",
    "    # 4. Gumbel Soft Thinking\n",
    "    print(\"\\n[Soft Thinking + Gumbel]\")\n",
    "    gumbel_result = generate_soft_thinking(\n",
    "        soft_gumbel_sampler, prefix,\n",
    "        CONFIG['max_new_tokens'],\n",
    "        CONFIG['soft_gumbel']['num_thinking_steps'],\n",
    "        'gumbel'\n",
    "    )\n",
    "    print(f\"  Answer: {gumbel_result['answer']}\")\n",
    "    print(f\"  Tokens: {gumbel_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {gumbel_result['cumulative_log_prob']:.4f}\")\n",
    "    print(f\"  Avg thinking steps: {gumbel_result['avg_thinking_steps']:.2f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'question_idx': idx,\n",
    "        'question': question,\n",
    "        'correct_answer': correct_answer,\n",
    "        \n",
    "        # Standard\n",
    "        'std': std_result,\n",
    "        'std_correct': std_result['answer'] == correct_answer,\n",
    "        \n",
    "        # Vanilla\n",
    "        'vanilla': vanilla_result,\n",
    "        'vanilla_correct': vanilla_result['answer'] == correct_answer,\n",
    "        \n",
    "        # Dirichlet\n",
    "        'dirichlet': dirichlet_result,\n",
    "        'dirichlet_correct': dirichlet_result['answer'] == correct_answer,\n",
    "        \n",
    "        # Gumbel\n",
    "        'gumbel': gumbel_result,\n",
    "        'gumbel_correct': gumbel_result['answer'] == correct_answer,\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n✓ Completed data collection for {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_file = os.path.join(CONFIG['save_dir'], 'comprehensive_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    # Convert to JSON-serializable format\n",
    "    json_results = []\n",
    "    for r in results:\n",
    "        json_r = {\n",
    "            'question_idx': r['question_idx'],\n",
    "            'question': r['question'],\n",
    "            'correct_answer': r['correct_answer'],\n",
    "            'std_correct': r['std_correct'],\n",
    "            'vanilla_correct': r['vanilla_correct'],\n",
    "            'dirichlet_correct': r['dirichlet_correct'],\n",
    "            'gumbel_correct': r['gumbel_correct'],\n",
    "        }\n",
    "        for method in ['std', 'vanilla', 'dirichlet', 'gumbel']:\n",
    "            for key in ['answer', 'num_tokens', 'cumulative_log_prob', 'mean_log_prob']:\n",
    "                if key in r[method]:\n",
    "                    json_r[f'{method}_{key}'] = r[method][key]\n",
    "            if method != 'std':\n",
    "                json_r[f'{method}_avg_thinking_steps'] = r[method]['avg_thinking_steps']\n",
    "                json_r[f'{method}_total_thinking_steps'] = r[method]['total_thinking_steps']\n",
    "        json_results.append(json_r)\n",
    "    \n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved results to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis\n",
    "\n",
    "Compare all four methods across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        'question_idx': r['question_idx'],\n",
    "        \n",
    "        # Standard\n",
    "        'std_log_prob': r['std']['cumulative_log_prob'],\n",
    "        'std_tokens': r['std']['num_tokens'],\n",
    "        'std_correct': r['std_correct'],\n",
    "        \n",
    "        # Vanilla\n",
    "        'vanilla_log_prob': r['vanilla']['cumulative_log_prob'],\n",
    "        'vanilla_tokens': r['vanilla']['num_tokens'],\n",
    "        'vanilla_correct': r['vanilla_correct'],\n",
    "        'vanilla_thinking_steps': r['vanilla']['avg_thinking_steps'],\n",
    "        \n",
    "        # Dirichlet\n",
    "        'dirichlet_log_prob': r['dirichlet']['cumulative_log_prob'],\n",
    "        'dirichlet_tokens': r['dirichlet']['num_tokens'],\n",
    "        'dirichlet_correct': r['dirichlet_correct'],\n",
    "        'dirichlet_thinking_steps': r['dirichlet']['avg_thinking_steps'],\n",
    "        \n",
    "        # Gumbel\n",
    "        'gumbel_log_prob': r['gumbel']['cumulative_log_prob'],\n",
    "        'gumbel_tokens': r['gumbel']['num_tokens'],\n",
    "        'gumbel_correct': r['gumbel_correct'],\n",
    "        'gumbel_thinking_steps': r['gumbel']['avg_thinking_steps'],\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "methods = ['std', 'vanilla', 'dirichlet', 'gumbel']\n",
    "method_names = ['Standard', 'Vanilla Soft', 'Dirichlet Soft', 'Gumbel Soft']\n",
    "\n",
    "for method, name in zip(methods, method_names):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {df_summary[f'{method}_correct'].mean():.2%}\")\n",
    "    print(f\"  Mean log-prob: {df_summary[f'{method}_log_prob'].mean():.4f}\")\n",
    "    print(f\"  Mean tokens: {df_summary[f'{method}_tokens'].mean():.2f}\")\n",
    "    if method != 'std':\n",
    "        print(f\"  Avg thinking steps: {df_summary[f'{method}_thinking_steps'].mean():.2f}\")\n",
    "\n",
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis\n",
    "\n",
    "Use Friedman test (non-parametric) for comparing multiple related samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman test for log-probabilities\n",
    "stat, p_value = friedmanchisquare(\n",
    "    df_summary['std_log_prob'],\n",
    "    df_summary['vanilla_log_prob'],\n",
    "    df_summary['dirichlet_log_prob'],\n",
    "    df_summary['gumbel_log_prob']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRIEDMAN TEST (Multiple Related Samples)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTesting: Are the log-probabilities different across methods?\")\n",
    "print(f\"  Test statistic: {stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀: Methods produce significantly different results (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀: No significant difference (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# Pairwise comparisons (Wilcoxon)\n",
    "print(\"\\nPairwise Wilcoxon Tests:\")\n",
    "pairs = [\n",
    "    ('Standard', 'Vanilla Soft', 'std_log_prob', 'vanilla_log_prob'),\n",
    "    ('Vanilla Soft', 'Dirichlet Soft', 'vanilla_log_prob', 'dirichlet_log_prob'),\n",
    "    ('Vanilla Soft', 'Gumbel Soft', 'vanilla_log_prob', 'gumbel_log_prob'),\n",
    "    ('Dirichlet Soft', 'Gumbel Soft', 'dirichlet_log_prob', 'gumbel_log_prob'),\n",
    "]\n",
    "\n",
    "for name1, name2, col1, col2 in pairs:\n",
    "    stat_w, p_w = wilcoxon(df_summary[col1], df_summary[col2])\n",
    "    sig = \"✓\" if p_w < CONFIG['alpha'] else \"✗\"\n",
    "    print(f\"  {sig} {name1} vs {name2}: p={p_w:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization dashboard\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "accuracies = [df_summary[f'{m}_correct'].mean() for m in methods]\n",
    "ax1.bar(method_names, accuracies, alpha=0.7, color=['gray', 'blue', 'green', 'orange'])\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Log-prob box plot\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "log_prob_data = [df_summary[f'{m}_log_prob'] for m in methods]\n",
    "ax2.boxplot(log_prob_data, labels=method_names)\n",
    "ax2.set_ylabel('Cumulative Log-Prob')\n",
    "ax2.set_title('Log-Probability Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# 3. Token efficiency\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "token_means = [df_summary[f'{m}_tokens'].mean() for m in methods]\n",
    "ax3.bar(method_names, token_means, alpha=0.7, color=['gray', 'blue', 'green', 'orange'])\n",
    "ax3.set_ylabel('Average Tokens')\n",
    "ax3.set_title('Token Efficiency')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Thinking steps comparison\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "thinking_steps = [df_summary[f'{m}_thinking_steps'].mean() for m in methods[1:]]  # Exclude standard\n",
    "ax4.bar(method_names[1:], thinking_steps, alpha=0.7, color=['blue', 'green', 'orange'])\n",
    "ax4.set_ylabel('Avg Thinking Steps')\n",
    "ax4.set_title('Thinking Steps per Token')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Log-prob scatter: Standard vs each Soft method\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "for i, (method, name, color) in enumerate(zip(methods[1:], method_names[1:], ['blue', 'green', 'orange'])):\n",
    "    ax5.scatter(df_summary['std_log_prob'], df_summary[f'{method}_log_prob'],\n",
    "               alpha=0.6, label=name, color=color, s=50)\n",
    "lims = [df_summary[[f'{m}_log_prob' for m in methods]].min().min(),\n",
    "        df_summary[[f'{m}_log_prob' for m in methods]].max().max()]\n",
    "ax5.plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "ax5.set_xlabel('Standard Log-Prob')\n",
    "ax5.set_ylabel('Soft Thinking Log-Prob')\n",
    "ax5.set_title('Standard vs Soft Thinking')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Improvement heatmap\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "improvements = np.zeros((len(methods), len(methods)))\n",
    "for i, m1 in enumerate(methods):\n",
    "    for j, m2 in enumerate(methods):\n",
    "        improvements[i, j] = (df_summary[f'{m2}_log_prob'].mean() - df_summary[f'{m1}_log_prob'].mean())\n",
    "im = ax6.imshow(improvements, cmap='RdYlGn', aspect='auto', vmin=-50, vmax=50)\n",
    "ax6.set_xticks(range(len(methods)))\n",
    "ax6.set_yticks(range(len(methods)))\n",
    "ax6.set_xticklabels(method_names, rotation=45, ha='right')\n",
    "ax6.set_yticklabels(method_names)\n",
    "ax6.set_title('Log-Prob Improvement Matrix\\n(Row - Column)')\n",
    "plt.colorbar(im, ax=ax6)\n",
    "\n",
    "# 7-9. Distribution histograms for each soft method\n",
    "for idx, (method, name, color) in enumerate(zip(methods[1:], method_names[1:], ['blue', 'green', 'orange'])):\n",
    "    ax = fig.add_subplot(gs[2, idx])\n",
    "    diff = df_summary[f'{method}_log_prob'] - df_summary['std_log_prob']\n",
    "    ax.hist(diff, bins=15, alpha=0.7, color=color, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='No improvement')\n",
    "    ax.axvline(diff.mean(), color='green', linestyle='--', linewidth=2,\n",
    "              label=f'Mean={diff.mean():.2f}')\n",
    "    ax.set_xlabel('Log-Prob Improvement')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{name} vs Standard')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'comprehensive_dashboard.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization dashboard generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "COMPREHENSIVE SOFT THINKING ANALYSIS\n",
    "Standard vs Vanilla vs Dirichlet vs Gumbel-Softmax\n",
    "{'='*80}\n",
    "\n",
    "1. CONFIGURATION\n",
    "{'-'*80}\n",
    "Model: {CONFIG['model_str']}\n",
    "Questions: {len(results)}\n",
    "Methods:\n",
    "  - Standard Sampling\n",
    "  - Vanilla Soft Thinking (no noise)\n",
    "  - Dirichlet Soft Thinking (alpha={CONFIG['soft_dirichlet']['noise_alpha']})\n",
    "  - Gumbel-Softmax Soft Thinking (tau={CONFIG['soft_gumbel']['gumbel_tau']})\n",
    "\n",
    "2. PERFORMANCE METRICS\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "for method, name in zip(methods, method_names):\n",
    "    report += f\"\"\"\n",
    "{name}:\n",
    "  Accuracy: {df_summary[f'{method}_correct'].mean():.2%} ({df_summary[f'{method}_correct'].sum()}/{len(df_summary)})\n",
    "  Mean log-prob: {df_summary[f'{method}_log_prob'].mean():.4f}\n",
    "  Mean tokens: {df_summary[f'{method}_tokens'].mean():.2f}\n",
    "\"\"\"\n",
    "    if method != 'std':\n",
    "        report += f\"  Avg thinking steps: {df_summary[f'{method}_thinking_steps'].mean():.2f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "3. STATISTICAL ANALYSIS\n",
    "{'-'*80}\n",
    "Friedman Test (4 methods):\n",
    "  Test statistic: {stat:.4f}\n",
    "  p-value: {p_value:.6f}\n",
    "  Result: {'SIGNIFICANT' if p_value < CONFIG['alpha'] else 'NOT SIGNIFICANT'}\n",
    "\n",
    "4. KEY FINDINGS\n",
    "{'-'*80}\n",
    "Best Accuracy: {max([(name, df_summary[f'{m}_correct'].mean()) for m, name in zip(methods, method_names)], key=lambda x: x[1])[0]}\n",
    "Best Log-Prob: {max([(name, df_summary[f'{m}_log_prob'].mean()) for m, name in zip(methods, method_names)], key=lambda x: x[1])[0]}\n",
    "Most Efficient (tokens): {min([(name, df_summary[f'{m}_tokens'].mean()) for m, name in zip(methods, method_names)], key=lambda x: x[1])[0]}\n",
    "\n",
    "5. NOISE INJECTION ANALYSIS\n",
    "{'-'*80}\n",
    "Dirichlet vs Vanilla:\n",
    "  Accuracy diff: {(df_summary['dirichlet_correct'].mean() - df_summary['vanilla_correct'].mean()):.2%}\n",
    "  Log-prob diff: {(df_summary['dirichlet_log_prob'].mean() - df_summary['vanilla_log_prob'].mean()):.4f}\n",
    "\n",
    "Gumbel vs Vanilla:\n",
    "  Accuracy diff: {(df_summary['gumbel_correct'].mean() - df_summary['vanilla_correct'].mean()):.2%}\n",
    "  Log-prob diff: {(df_summary['gumbel_log_prob'].mean() - df_summary['vanilla_log_prob'].mean()):.4f}\n",
    "\n",
    "6. CONCLUSION\n",
    "{'-'*80}\n",
    "Enhanced Soft Thinking with noise injection {'demonstrates' if p_value < CONFIG['alpha'] else 'shows'} \n",
    "{'statistically significant' if p_value < CONFIG['alpha'] else 'no significant'} differences\n",
    "compared to standard sampling. The dual temperature control and multi-criteria\n",
    "early stopping provide fine-grained control over the thinking process.\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_file = os.path.join(CONFIG['save_dir'], 'comprehensive_report.txt')\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n✓ Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary DataFrame\n",
    "df_file = os.path.join(CONFIG['save_dir'], 'comprehensive_summary.csv')\n",
    "df_summary.to_csv(df_file, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll results saved to: {CONFIG['save_dir']}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for filename in os.listdir(CONFIG['save_dir']):\n",
    "    filepath = os.path.join(CONFIG['save_dir'], filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {filename} ({size:,} bytes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
