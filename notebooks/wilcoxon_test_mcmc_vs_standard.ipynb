{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilcoxon Signed Rank Test: MCMC vs Standard Sampling on MATH 500\n",
    "\n",
    "This notebook performs a comprehensive statistical analysis comparing MCMC power sampling against standard sampling for mathematical reasoning tasks. We analyze:\n",
    "\n",
    "1. **Data Collection**: Detailed extraction of answers, reasoning steps, tokens, and log-probabilities\n",
    "2. **Wilcoxon Signed Rank Test**: Non-parametric paired comparison of log-probabilities\n",
    "3. **Statistical Power Analysis**: Type I error, Type II error, and power simulations\n",
    "4. **Effect Size Analysis**: Understanding practical significance of differences\n",
    "\n",
    "## Background: Wilcoxon Signed Rank Test\n",
    "\n",
    "The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used to compare two related samples (paired observations). It's particularly useful when:\n",
    "- Data is paired (same questions answered by both methods)\n",
    "- Distribution may not be normal\n",
    "- We want to test if median differences are zero\n",
    "\n",
    "**Null Hypothesis (H₀)**: The median difference in log-probabilities between MCMC and standard sampling is zero\n",
    "\n",
    "**Alternative Hypothesis (H₁)**: The median difference is not zero (two-tailed) or MCMC > Standard (one-tailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, shapiro, normaltest\n",
    "\n",
    "# PyTorch for model inference\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# Add project path to sys.path\n",
    "project_root = Path('/home/user/reasoning-with-sampling/llm_experiments')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from grader_utils.parse_utils import parse_answer\n",
    "from constants import *\n",
    "from power_samp_utils import AutoregressiveSampler, mcmc_power_samp, format_prompt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "CONFIG = {\n",
    "    'model': 'qwen_math',  # Options: qwen, qwen_math, phi, tulu\n",
    "    'model_str': 'Qwen/Qwen2.5-Math-7B',\n",
    "    'dataset': 'MATH',\n",
    "    'dataset_path': '/home/user/reasoning-with-sampling/llm_experiments/data/MATH500.json',\n",
    "    'temperature': 0.25,\n",
    "    'mcmc_steps': 10,\n",
    "    'max_new_tokens': 3072,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_questions': 20,  # Start with small subset for testing; increase for full analysis\n",
    "    'cot': True,\n",
    "    'save_dir': '/home/user/reasoning-with-sampling/notebooks/results',\n",
    "    'alpha': 0.05,  # Significance level for statistical tests\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(CONFIG['dataset_path'], 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} problems from MATH500 dataset\")\n",
    "print(f\"Will process {CONFIG['num_questions']} questions for this analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {CONFIG['model_str']}\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model_str'], \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_str'],\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "autoreg_sampler = AutoregressiveSampler(model, tokenizer, CONFIG['device'])\n",
    "\n",
    "print(f\"Model loaded successfully on {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection Functions\n",
    "\n",
    "These functions collect detailed information about each generation including:\n",
    "- Full completion text\n",
    "- Parsed answer\n",
    "- Token IDs and decoded tokens\n",
    "- Individual token log-probabilities\n",
    "- Cumulative log-probability\n",
    "- Number of tokens generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_logprobs(output, input_length: int, tokenizer) -> Tuple[List[float], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract per-token log probabilities and tokens from model output.\n",
    "    \n",
    "    Args:\n",
    "        output: Model generation output with scores\n",
    "        input_length: Length of input tokens to skip\n",
    "        tokenizer: Tokenizer for decoding\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (log_probs, tokens)\n",
    "    \"\"\"\n",
    "    generated_ids = output.sequences[0][input_length:]\n",
    "    scores = output.scores  # List of tensors, one per generated token\n",
    "    \n",
    "    log_probs = []\n",
    "    tokens = []\n",
    "    \n",
    "    for i, token_id in enumerate(generated_ids):\n",
    "        if i < len(scores):\n",
    "            # Get log probabilities for this position\n",
    "            logits = scores[i][0]  # First batch element\n",
    "            log_prob_dist = F.log_softmax(logits, dim=-1)\n",
    "            token_log_prob = log_prob_dist[token_id].item()\n",
    "            log_probs.append(token_log_prob)\n",
    "        \n",
    "        # Decode token\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        tokens.append(token_text)\n",
    "    \n",
    "    return log_probs, tokens\n",
    "\n",
    "\n",
    "def generate_standard(model, tokenizer, input_ids, device, max_new_tokens=3072) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate using standard sampling (temperature=1.0).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with completion, answer, tokens, log_probs, and statistics\n",
    "    \"\"\"\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    \n",
    "    # Extract tokens and log probs\n",
    "    generated_ids = output.sequences[0][len(input_ids[0]):]\n",
    "    completion = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    \n",
    "    # Get detailed token information\n",
    "    log_probs, tokens = extract_token_logprobs(output, len(input_ids[0]), tokenizer)\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids.cpu().tolist(),\n",
    "        'log_probs': log_probs,\n",
    "        'cumulative_log_prob': sum(log_probs) if log_probs else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs) if log_probs else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_mcmc(autoreg_sampler, prefix, temp, mcmc_steps, max_new_tokens=3072) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate using MCMC power sampling.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with completion, answer, tokens, log_probs, and statistics\n",
    "    \"\"\"\n",
    "    token_ids, log_probs_norm, log_probs_unnorm, acceptance_ratio = mcmc_power_samp(\n",
    "        autoreg_sampler, prefix, temp, mcmc_steps, max_new_tokens\n",
    "    )\n",
    "    \n",
    "    # Remove prefix from token_ids to get only generated tokens\n",
    "    generated_ids = token_ids[len(prefix):]\n",
    "    \n",
    "    # Decode tokens\n",
    "    completion = autoreg_sampler.tokenizer.decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    \n",
    "    # Decode individual tokens\n",
    "    tokens = [autoreg_sampler.tokenizer.decode([tid]) for tid in generated_ids]\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids,\n",
    "        'log_probs': log_probs_unnorm,  # Use unnormalized (target distribution) log probs\n",
    "        'log_probs_proposal': log_probs_norm,  # Proposal distribution log probs\n",
    "        'cumulative_log_prob': sum(log_probs_unnorm) if log_probs_unnorm else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs_unnorm) if log_probs_unnorm else 0.0,\n",
    "        'acceptance_ratio': acceptance_ratio,\n",
    "    }\n",
    "\n",
    "\n",
    "def split_into_steps(completion: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split completion into reasoning steps.\n",
    "    \n",
    "    Simple heuristic: split by newlines and filter empty lines.\n",
    "    For more sophisticated parsing, could use regex or model-specific patterns.\n",
    "    \"\"\"\n",
    "    steps = [line.strip() for line in completion.split('\\n') if line.strip()]\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments and Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection\n",
    "results = []\n",
    "\n",
    "# Process subset of questions\n",
    "questions_to_process = dataset[:CONFIG['num_questions']]\n",
    "\n",
    "for idx, data in enumerate(tqdm(questions_to_process, desc=\"Processing MATH problems\")):\n",
    "    question = data['prompt']\n",
    "    correct_answer = data['answer']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {idx+1}/{len(questions_to_process)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Q: {question[:100]}...\")\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = format_prompt(question, CONFIG['model'], tokenizer, CONFIG['cot'])\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(CONFIG['device'])\n",
    "    prefix = [idx.item() for idx in input_ids[0]]\n",
    "    \n",
    "    # Standard sampling\n",
    "    print(\"\\n[Standard Sampling]\")\n",
    "    std_result = generate_standard(\n",
    "        model, tokenizer, input_ids, CONFIG['device'], CONFIG['max_new_tokens']\n",
    "    )\n",
    "    print(f\"  Answer: {std_result['answer']}\")\n",
    "    print(f\"  Tokens: {std_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {std_result['cumulative_log_prob']:.4f}\")\n",
    "    \n",
    "    # MCMC sampling\n",
    "    print(\"\\n[MCMC Sampling]\")\n",
    "    mcmc_result = generate_mcmc(\n",
    "        autoreg_sampler, prefix, CONFIG['temperature'], \n",
    "        CONFIG['mcmc_steps'], CONFIG['max_new_tokens']\n",
    "    )\n",
    "    print(f\"  Answer: {mcmc_result['answer']}\")\n",
    "    print(f\"  Tokens: {mcmc_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {mcmc_result['cumulative_log_prob']:.4f}\")\n",
    "    print(f\"  Acceptance ratio: {mcmc_result['acceptance_ratio']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'question_idx': idx,\n",
    "        'question': question,\n",
    "        'correct_answer': correct_answer,\n",
    "        \n",
    "        # Standard sampling results\n",
    "        'std_completion': std_result['completion'],\n",
    "        'std_answer': std_result['answer'],\n",
    "        'std_steps': split_into_steps(std_result['completion']),\n",
    "        'std_tokens': std_result['tokens'],\n",
    "        'std_token_ids': std_result['token_ids'],\n",
    "        'std_log_probs': std_result['log_probs'],\n",
    "        'std_cumulative_log_prob': std_result['cumulative_log_prob'],\n",
    "        'std_num_tokens': std_result['num_tokens'],\n",
    "        'std_mean_log_prob': std_result['mean_log_prob'],\n",
    "        'std_correct': std_result['answer'] == correct_answer,\n",
    "        \n",
    "        # MCMC sampling results\n",
    "        'mcmc_completion': mcmc_result['completion'],\n",
    "        'mcmc_answer': mcmc_result['answer'],\n",
    "        'mcmc_steps': split_into_steps(mcmc_result['completion']),\n",
    "        'mcmc_tokens': mcmc_result['tokens'],\n",
    "        'mcmc_token_ids': mcmc_result['token_ids'],\n",
    "        'mcmc_log_probs': mcmc_result['log_probs'],\n",
    "        'mcmc_log_probs_proposal': mcmc_result['log_probs_proposal'],\n",
    "        'mcmc_cumulative_log_prob': mcmc_result['cumulative_log_prob'],\n",
    "        'mcmc_num_tokens': mcmc_result['num_tokens'],\n",
    "        'mcmc_mean_log_prob': mcmc_result['mean_log_prob'],\n",
    "        'mcmc_acceptance_ratio': mcmc_result['acceptance_ratio'],\n",
    "        'mcmc_correct': mcmc_result['answer'] == correct_answer,\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\nCompleted data collection for {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "results_file = os.path.join(CONFIG['save_dir'], 'wilcoxon_raw_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved raw results to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        'question_idx': r['question_idx'],\n",
    "        'std_cumulative_log_prob': r['std_cumulative_log_prob'],\n",
    "        'mcmc_cumulative_log_prob': r['mcmc_cumulative_log_prob'],\n",
    "        'std_mean_log_prob': r['std_mean_log_prob'],\n",
    "        'mcmc_mean_log_prob': r['mcmc_mean_log_prob'],\n",
    "        'std_num_tokens': r['std_num_tokens'],\n",
    "        'mcmc_num_tokens': r['mcmc_num_tokens'],\n",
    "        'std_correct': r['std_correct'],\n",
    "        'mcmc_correct': r['mcmc_correct'],\n",
    "        'mcmc_acceptance_ratio': r['mcmc_acceptance_ratio'],\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStandard Sampling:\")\n",
    "print(f\"  Mean cumulative log-prob: {df_summary['std_cumulative_log_prob'].mean():.4f}\")\n",
    "print(f\"  Std cumulative log-prob: {df_summary['std_cumulative_log_prob'].std():.4f}\")\n",
    "print(f\"  Mean tokens: {df_summary['std_num_tokens'].mean():.2f}\")\n",
    "print(f\"  Accuracy: {df_summary['std_correct'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nMCMC Sampling:\")\n",
    "print(f\"  Mean cumulative log-prob: {df_summary['mcmc_cumulative_log_prob'].mean():.4f}\")\n",
    "print(f\"  Std cumulative log-prob: {df_summary['mcmc_cumulative_log_prob'].std():.4f}\")\n",
    "print(f\"  Mean tokens: {df_summary['mcmc_num_tokens'].mean():.2f}\")\n",
    "print(f\"  Accuracy: {df_summary['mcmc_correct'].mean():.2%}\")\n",
    "print(f\"  Mean acceptance ratio: {df_summary['mcmc_acceptance_ratio'].mean():.4f}\")\n",
    "\n",
    "# Calculate differences\n",
    "df_summary['log_prob_diff'] = df_summary['mcmc_cumulative_log_prob'] - df_summary['std_cumulative_log_prob']\n",
    "df_summary['mean_log_prob_diff'] = df_summary['mcmc_mean_log_prob'] - df_summary['std_mean_log_prob']\n",
    "\n",
    "print(\"\\nDifferences (MCMC - Standard):\")\n",
    "print(f\"  Mean cumulative log-prob difference: {df_summary['log_prob_diff'].mean():.4f}\")\n",
    "print(f\"  Std cumulative log-prob difference: {df_summary['log_prob_diff'].std():.4f}\")\n",
    "print(f\"  Mean per-token log-prob difference: {df_summary['mean_log_prob_diff'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Cumulative log-prob comparison\n",
    "axes[0, 0].scatter(df_summary['std_cumulative_log_prob'], \n",
    "                   df_summary['mcmc_cumulative_log_prob'], alpha=0.6)\n",
    "lims = [min(df_summary['std_cumulative_log_prob'].min(), df_summary['mcmc_cumulative_log_prob'].min()),\n",
    "        max(df_summary['std_cumulative_log_prob'].max(), df_summary['mcmc_cumulative_log_prob'].max())]\n",
    "axes[0, 0].plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "axes[0, 0].set_xlabel('Standard Cumulative Log-Prob')\n",
    "axes[0, 0].set_ylabel('MCMC Cumulative Log-Prob')\n",
    "axes[0, 0].set_title('Cumulative Log-Prob Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution of differences\n",
    "axes[0, 1].hist(df_summary['log_prob_diff'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(0, color='r', linestyle='--', label='Zero difference')\n",
    "axes[0, 1].axvline(df_summary['log_prob_diff'].mean(), color='g', \n",
    "                   linestyle='--', label=f'Mean = {df_summary[\"log_prob_diff\"].mean():.2f}')\n",
    "axes[0, 1].set_xlabel('Log-Prob Difference (MCMC - Standard)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Differences')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Box plot comparison\n",
    "box_data = [df_summary['std_cumulative_log_prob'], df_summary['mcmc_cumulative_log_prob']]\n",
    "axes[0, 2].boxplot(box_data, labels=['Standard', 'MCMC'])\n",
    "axes[0, 2].set_ylabel('Cumulative Log-Prob')\n",
    "axes[0, 2].set_title('Distribution Comparison')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Mean per-token log-prob comparison\n",
    "axes[1, 0].scatter(df_summary['std_mean_log_prob'], \n",
    "                   df_summary['mcmc_mean_log_prob'], alpha=0.6)\n",
    "lims = [min(df_summary['std_mean_log_prob'].min(), df_summary['mcmc_mean_log_prob'].min()),\n",
    "        max(df_summary['std_mean_log_prob'].max(), df_summary['mcmc_mean_log_prob'].max())]\n",
    "axes[1, 0].plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "axes[1, 0].set_xlabel('Standard Mean Log-Prob')\n",
    "axes[1, 0].set_ylabel('MCMC Mean Log-Prob')\n",
    "axes[1, 0].set_title('Mean Per-Token Log-Prob Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Acceptance ratio distribution\n",
    "axes[1, 1].hist(df_summary['mcmc_acceptance_ratio'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(df_summary['mcmc_acceptance_ratio'].mean(), color='r', \n",
    "                   linestyle='--', label=f'Mean = {df_summary[\"mcmc_acceptance_ratio\"].mean():.2f}')\n",
    "axes[1, 1].set_xlabel('Acceptance Ratio')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('MCMC Acceptance Ratios')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Accuracy comparison\n",
    "accuracy_data = [\n",
    "    ['Standard', df_summary['std_correct'].sum(), len(df_summary) - df_summary['std_correct'].sum()],\n",
    "    ['MCMC', df_summary['mcmc_correct'].sum(), len(df_summary) - df_summary['mcmc_correct'].sum()]\n",
    "]\n",
    "methods = [x[0] for x in accuracy_data]\n",
    "correct = [x[1] for x in accuracy_data]\n",
    "incorrect = [x[2] for x in accuracy_data]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "axes[1, 2].bar(x, correct, width, label='Correct', alpha=0.8)\n",
    "axes[1, 2].bar(x, incorrect, width, bottom=correct, label='Incorrect', alpha=0.8)\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].set_title('Accuracy Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(methods)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'eda_plots.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wilcoxon Signed Rank Test\n",
    "\n",
    "### Test Assumptions\n",
    "1. **Paired observations**: Same questions answered by both methods ✓\n",
    "2. **Continuous or ordinal data**: Log-probabilities are continuous ✓\n",
    "3. **Symmetric distribution of differences**: We'll check this below\n",
    "\n",
    "### Hypotheses\n",
    "- **H₀**: The median difference in cumulative log-probabilities is zero\n",
    "- **H₁**: The median difference is not zero (two-tailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normality of differences (to justify using Wilcoxon over t-test)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING ASSUMPTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "stat_shapiro, p_shapiro = shapiro(df_summary['log_prob_diff'])\n",
    "print(f\"\\nShapiro-Wilk Test for Normality of Differences:\")\n",
    "print(f\"  Test statistic: {stat_shapiro:.4f}\")\n",
    "print(f\"  p-value: {p_shapiro:.4f}\")\n",
    "if p_shapiro < CONFIG['alpha']:\n",
    "    print(f\"  → Reject normality (p < {CONFIG['alpha']}): Wilcoxon test is appropriate\")\n",
    "else:\n",
    "    print(f\"  → Cannot reject normality (p >= {CONFIG['alpha']}): Both tests could be used\")\n",
    "\n",
    "# Q-Q plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df_summary['log_prob_diff'], dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title('Q-Q Plot of Differences')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with normal overlay\n",
    "axes[1].hist(df_summary['log_prob_diff'], bins=20, density=True, \n",
    "             edgecolor='black', alpha=0.7, label='Observed')\n",
    "mu, std = df_summary['log_prob_diff'].mean(), df_summary['log_prob_diff'].std()\n",
    "x = np.linspace(df_summary['log_prob_diff'].min(), df_summary['log_prob_diff'].max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, mu, std), 'r-', lw=2, label='Normal fit')\n",
    "axes[1].set_xlabel('Log-Prob Difference')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Distribution of Differences vs Normal')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'normality_check.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Wilcoxon signed-rank test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WILCOXON SIGNED RANK TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Two-tailed test\n",
    "statistic, p_value = wilcoxon(\n",
    "    df_summary['mcmc_cumulative_log_prob'],\n",
    "    df_summary['std_cumulative_log_prob'],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "print(f\"\\nTwo-Tailed Test:\")\n",
    "print(f\"  H₀: median(MCMC - Standard) = 0\")\n",
    "print(f\"  H₁: median(MCMC - Standard) ≠ 0\")\n",
    "print(f\"  Test statistic (W): {statistic:.4f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "print(f\"  Significance level (α): {CONFIG['alpha']}\")\n",
    "\n",
    "if p_value < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀: Significant difference detected (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀: No significant difference (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# One-tailed test (MCMC > Standard)\n",
    "statistic_gt, p_value_gt = wilcoxon(\n",
    "    df_summary['mcmc_cumulative_log_prob'],\n",
    "    df_summary['std_cumulative_log_prob'],\n",
    "    alternative='greater'\n",
    ")\n",
    "\n",
    "print(f\"\\nOne-Tailed Test (MCMC > Standard):\")\n",
    "print(f\"  H₀: median(MCMC - Standard) ≤ 0\")\n",
    "print(f\"  H₁: median(MCMC - Standard) > 0\")\n",
    "print(f\"  Test statistic (W): {statistic_gt:.4f}\")\n",
    "print(f\"  p-value: {p_value_gt:.6f}\")\n",
    "\n",
    "if p_value_gt < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀: MCMC significantly better (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀: MCMC not significantly better (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# Effect size (rank-biserial correlation)\n",
    "n = len(df_summary)\n",
    "r = 1 - (4 * statistic) / (n * (n + 1))\n",
    "print(f\"\\nEffect Size (rank-biserial correlation r):\")\n",
    "print(f\"  r = {r:.4f}\")\n",
    "print(f\"  Interpretation: \", end=\"\")\n",
    "if abs(r) < 0.1:\n",
    "    print(\"Negligible\")\n",
    "elif abs(r) < 0.3:\n",
    "    print(\"Small\")\n",
    "elif abs(r) < 0.5:\n",
    "    print(\"Medium\")\n",
    "else:\n",
    "    print(\"Large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with paired t-test for reference\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRED T-TEST (FOR COMPARISON)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "t_stat, t_pvalue = stats.ttest_rel(\n",
    "    df_summary['mcmc_cumulative_log_prob'],\n",
    "    df_summary['std_cumulative_log_prob']\n",
    ")\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {t_pvalue:.6f}\")\n",
    "\n",
    "if t_pvalue < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀ (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀ (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# Cohen's d effect size\n",
    "cohens_d = df_summary['log_prob_diff'].mean() / df_summary['log_prob_diff'].std()\n",
    "print(f\"\\nCohen's d: {cohens_d:.4f}\")\n",
    "print(f\"  Interpretation: \", end=\"\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    print(\"Small\")\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    print(\"Medium\")\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    print(\"Large\")\n",
    "else:\n",
    "    print(\"Very Large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Power Analysis\n",
    "\n",
    "### Type I and Type II Errors\n",
    "\n",
    "- **Type I Error (α)**: False positive - rejecting H₀ when it's true\n",
    "  - Probability of concluding there's a difference when there isn't\n",
    "  - Controlled by significance level (typically 0.05)\n",
    "\n",
    "- **Type II Error (β)**: False negative - failing to reject H₀ when it's false\n",
    "  - Probability of missing a real difference\n",
    "  \n",
    "- **Statistical Power (1-β)**: Probability of correctly rejecting H₀ when it's false\n",
    "  - Power of 0.8 (80%) is typically considered adequate\n",
    "\n",
    "### Power Analysis via Simulation\n",
    "\n",
    "We'll simulate the power of the Wilcoxon test under different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_type_i_error(n_samples, n_simulations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Simulate Type I error rate under null hypothesis.\n",
    "    \n",
    "    Under H₀, both samples come from the same distribution.\n",
    "    We should reject H₀ only α% of the time.\n",
    "    \"\"\"\n",
    "    rejections = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate paired samples from same distribution (H₀ is true)\n",
    "        sample1 = np.random.normal(0, 1, n_samples)\n",
    "        sample2 = np.random.normal(0, 1, n_samples)  # Same distribution\n",
    "        \n",
    "        # Wilcoxon test\n",
    "        _, p_value = wilcoxon(sample1, sample2, alternative='two-sided')\n",
    "        \n",
    "        if p_value < alpha:\n",
    "            rejections += 1\n",
    "    \n",
    "    type_i_error_rate = rejections / n_simulations\n",
    "    return type_i_error_rate\n",
    "\n",
    "\n",
    "def simulate_power(n_samples, effect_size, n_simulations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Simulate statistical power under alternative hypothesis.\n",
    "    \n",
    "    Under H₁, samples come from different distributions.\n",
    "    Power is the probability of correctly rejecting H₀.\n",
    "    \n",
    "    Args:\n",
    "        effect_size: Mean difference in standard deviations (Cohen's d)\n",
    "    \"\"\"\n",
    "    rejections = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate paired samples with true difference (H₁ is true)\n",
    "        sample1 = np.random.normal(0, 1, n_samples)\n",
    "        sample2 = np.random.normal(effect_size, 1, n_samples)  # Shifted by effect_size\n",
    "        \n",
    "        # Wilcoxon test\n",
    "        _, p_value = wilcoxon(sample1, sample2, alternative='two-sided')\n",
    "        \n",
    "        if p_value < alpha:\n",
    "            rejections += 1\n",
    "    \n",
    "    power = rejections / n_simulations\n",
    "    return power\n",
    "\n",
    "\n",
    "def simulate_type_ii_error(n_samples, effect_size, n_simulations=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Simulate Type II error rate under alternative hypothesis.\n",
    "    \n",
    "    Type II error = 1 - Power\n",
    "    \"\"\"\n",
    "    power = simulate_power(n_samples, effect_size, n_simulations, alpha)\n",
    "    return 1 - power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Type I error\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TYPE I ERROR SIMULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_sims = 1000\n",
    "sample_size = len(df_summary)\n",
    "\n",
    "print(f\"\\nSimulating {n_sims} tests under H₀ (no true difference)...\")\n",
    "type_i_error = simulate_type_i_error(sample_size, n_sims, CONFIG['alpha'])\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Nominal α (significance level): {CONFIG['alpha']:.3f}\")\n",
    "print(f\"  Simulated Type I error rate: {type_i_error:.3f}\")\n",
    "print(f\"  Expected rejections: {CONFIG['alpha'] * n_sims:.0f}\")\n",
    "print(f\"  Actual rejections: {type_i_error * n_sims:.0f}\")\n",
    "\n",
    "if abs(type_i_error - CONFIG['alpha']) < 0.02:  # Within 2%\n",
    "    print(f\"  ✓ Type I error rate matches nominal α\")\n",
    "else:\n",
    "    print(f\"  ⚠ Type I error rate differs from nominal α (may be due to sampling variation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis across different effect sizes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL POWER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "effect_sizes = np.linspace(0, 1.5, 16)  # Cohen's d from 0 to 1.5\n",
    "powers = []\n",
    "type_ii_errors = []\n",
    "\n",
    "print(f\"\\nSimulating power across effect sizes (n={sample_size}, α={CONFIG['alpha']})...\")\n",
    "for es in tqdm(effect_sizes, desc=\"Effect sizes\"):\n",
    "    power = simulate_power(sample_size, es, n_simulations=500, alpha=CONFIG['alpha'])\n",
    "    powers.append(power)\n",
    "    type_ii_errors.append(1 - power)\n",
    "\n",
    "# Plot power curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Power curve\n",
    "axes[0].plot(effect_sizes, powers, 'b-', linewidth=2, label='Statistical Power (1-β)')\n",
    "axes[0].axhline(y=0.8, color='r', linestyle='--', label='Target Power (0.8)', alpha=0.7)\n",
    "axes[0].axhline(y=CONFIG['alpha'], color='g', linestyle='--', \n",
    "                label=f'Type I Error (α={CONFIG[\"alpha\"]})', alpha=0.7)\n",
    "axes[0].fill_between(effect_sizes, 0, powers, alpha=0.2)\n",
    "axes[0].set_xlabel('Effect Size (Cohen\\'s d)', fontsize=12)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_title('Statistical Power vs Effect Size', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Type II error curve\n",
    "axes[1].plot(effect_sizes, type_ii_errors, 'r-', linewidth=2, label='Type II Error (β)')\n",
    "axes[1].axhline(y=0.2, color='b', linestyle='--', label='Target β (0.2)', alpha=0.7)\n",
    "axes[1].fill_between(effect_sizes, 0, type_ii_errors, alpha=0.2, color='red')\n",
    "axes[1].set_xlabel('Effect Size (Cohen\\'s d)', fontsize=12)\n",
    "axes[1].set_ylabel('Probability', fontsize=12)\n",
    "axes[1].set_title('Type II Error vs Effect Size', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'power_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find minimum detectable effect size for 80% power\n",
    "target_power = 0.8\n",
    "idx_80 = np.argmin(np.abs(np.array(powers) - target_power))\n",
    "mde = effect_sizes[idx_80]\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  Sample size: n = {sample_size}\")\n",
    "print(f\"  Minimum Detectable Effect (80% power): d ≈ {mde:.3f}\")\n",
    "print(f\"  Observed effect size in our data: d = {cohens_d:.3f}\")\n",
    "\n",
    "if abs(cohens_d) >= mde:\n",
    "    print(f\"  ✓ Our study has sufficient power to detect the observed effect\")\n",
    "else:\n",
    "    print(f\"  ⚠ Our study may be underpowered for the observed effect size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size analysis: How many samples needed for different effect sizes?\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE SIZE REQUIREMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_effect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large\n",
    "sample_sizes = [10, 20, 30, 50, 75, 100, 150, 200]\n",
    "\n",
    "power_results = {}\n",
    "for es in target_effect_sizes:\n",
    "    powers_for_es = []\n",
    "    for n in tqdm(sample_sizes, desc=f\"Effect size d={es}\", leave=False):\n",
    "        power = simulate_power(n, es, n_simulations=300, alpha=CONFIG['alpha'])\n",
    "        powers_for_es.append(power)\n",
    "    power_results[es] = powers_for_es\n",
    "\n",
    "# Plot sample size requirements\n",
    "plt.figure(figsize=(10, 6))\n",
    "for es, powers_list in power_results.items():\n",
    "    plt.plot(sample_sizes, powers_list, marker='o', linewidth=2, \n",
    "             label=f'Effect size d = {es}')\n",
    "\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='Target Power (0.8)', alpha=0.7)\n",
    "plt.xlabel('Sample Size (n)', fontsize=12)\n",
    "plt.ylabel('Statistical Power (1-β)', fontsize=12)\n",
    "plt.title('Sample Size Requirements for Different Effect Sizes', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'sample_size_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSample Size Recommendations (for 80% power, α={CONFIG['alpha']}):\")\n",
    "for es in target_effect_sizes:\n",
    "    powers_list = power_results[es]\n",
    "    # Find minimum sample size for 80% power\n",
    "    for i, (n, p) in enumerate(zip(sample_sizes, powers_list)):\n",
    "        if p >= 0.8:\n",
    "            print(f\"  Effect size d = {es:.1f}: n ≥ {n}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  Effect size d = {es:.1f}: n > {sample_sizes[-1]} (need larger sample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Per-Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results table\n",
    "detailed_results = pd.DataFrame([\n",
    "    {\n",
    "        'Q_ID': r['question_idx'],\n",
    "        'Question': r['question'][:50] + '...',\n",
    "        'Std_LogProb': r['std_cumulative_log_prob'],\n",
    "        'MCMC_LogProb': r['mcmc_cumulative_log_prob'],\n",
    "        'Diff': r['mcmc_cumulative_log_prob'] - r['std_cumulative_log_prob'],\n",
    "        'Std_Tokens': r['std_num_tokens'],\n",
    "        'MCMC_Tokens': r['mcmc_num_tokens'],\n",
    "        'Std_Correct': '✓' if r['std_correct'] else '✗',\n",
    "        'MCMC_Correct': '✓' if r['mcmc_correct'] else '✗',\n",
    "        'MCMC_AcceptRatio': r['mcmc_acceptance_ratio'],\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED PER-QUESTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(detailed_results.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "detailed_results.to_csv(\n",
    "    os.path.join(CONFIG['save_dir'], 'detailed_results.csv'), \n",
    "    index=False\n",
    ")\n",
    "print(f\"\\nSaved detailed results to {os.path.join(CONFIG['save_dir'], 'detailed_results.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "WILCOXON SIGNED RANK TEST: MCMC vs STANDARD SAMPLING\n",
    "COMPREHENSIVE STATISTICAL ANALYSIS REPORT\n",
    "{'='*80}\n",
    "\n",
    "1. EXPERIMENT CONFIGURATION\n",
    "{'-'*80}\n",
    "Model: {CONFIG['model']} ({CONFIG['model_str']})\n",
    "Dataset: MATH500\n",
    "Questions analyzed: {len(results)}\n",
    "Temperature: {CONFIG['temperature']}\n",
    "MCMC steps: {CONFIG['mcmc_steps']}\n",
    "Significance level (α): {CONFIG['alpha']}\n",
    "\n",
    "2. DESCRIPTIVE STATISTICS\n",
    "{'-'*80}\n",
    "Standard Sampling:\n",
    "  Mean cumulative log-prob: {df_summary['std_cumulative_log_prob'].mean():.4f} ± {df_summary['std_cumulative_log_prob'].std():.4f}\n",
    "  Mean tokens: {df_summary['std_num_tokens'].mean():.2f}\n",
    "  Accuracy: {df_summary['std_correct'].mean():.2%} ({df_summary['std_correct'].sum()}/{len(df_summary)})\n",
    "\n",
    "MCMC Sampling:\n",
    "  Mean cumulative log-prob: {df_summary['mcmc_cumulative_log_prob'].mean():.4f} ± {df_summary['mcmc_cumulative_log_prob'].std():.4f}\n",
    "  Mean tokens: {df_summary['mcmc_num_tokens'].mean():.2f}\n",
    "  Accuracy: {df_summary['mcmc_correct'].mean():.2%} ({df_summary['mcmc_correct'].sum()}/{len(df_summary)})\n",
    "  Mean acceptance ratio: {df_summary['mcmc_acceptance_ratio'].mean():.4f}\n",
    "\n",
    "Difference (MCMC - Standard):\n",
    "  Mean difference: {df_summary['log_prob_diff'].mean():.4f}\n",
    "  Median difference: {df_summary['log_prob_diff'].median():.4f}\n",
    "  Std difference: {df_summary['log_prob_diff'].std():.4f}\n",
    "\n",
    "3. WILCOXON SIGNED RANK TEST RESULTS\n",
    "{'-'*80}\n",
    "Two-tailed test:\n",
    "  H₀: median(MCMC - Standard) = 0\n",
    "  H₁: median(MCMC - Standard) ≠ 0\n",
    "  \n",
    "  Test statistic (W): {statistic:.4f}\n",
    "  p-value: {p_value:.6f}\n",
    "  Decision: {'REJECT H₀' if p_value < CONFIG['alpha'] else 'FAIL TO REJECT H₀'}\n",
    "  \n",
    "  Effect size (rank-biserial r): {r:.4f}\n",
    "  Interpretation: {'Negligible' if abs(r) < 0.1 else 'Small' if abs(r) < 0.3 else 'Medium' if abs(r) < 0.5 else 'Large'}\n",
    "\n",
    "One-tailed test (MCMC > Standard):\n",
    "  p-value: {p_value_gt:.6f}\n",
    "  Decision: {'REJECT H₀' if p_value_gt < CONFIG['alpha'] else 'FAIL TO REJECT H₀'}\n",
    "\n",
    "4. PAIRED T-TEST (PARAMETRIC COMPARISON)\n",
    "{'-'*80}\n",
    "  t-statistic: {t_stat:.4f}\n",
    "  p-value: {t_pvalue:.6f}\n",
    "  Decision: {'REJECT H₀' if t_pvalue < CONFIG['alpha'] else 'FAIL TO REJECT H₀'}\n",
    "  \n",
    "  Cohen's d: {cohens_d:.4f}\n",
    "  Interpretation: {'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'}\n",
    "\n",
    "5. NORMALITY ASSESSMENT\n",
    "{'-'*80}\n",
    "Shapiro-Wilk test:\n",
    "  Test statistic: {stat_shapiro:.4f}\n",
    "  p-value: {p_shapiro:.4f}\n",
    "  Conclusion: {'Non-normal distribution (Wilcoxon preferred)' if p_shapiro < CONFIG['alpha'] else 'Cannot reject normality'}\n",
    "\n",
    "6. POWER ANALYSIS\n",
    "{'-'*80}\n",
    "Type I Error Simulation:\n",
    "  Nominal α: {CONFIG['alpha']:.3f}\n",
    "  Simulated Type I error: {type_i_error:.3f}\n",
    "  Status: {'✓ Matches nominal α' if abs(type_i_error - CONFIG['alpha']) < 0.02 else '⚠ Differs from nominal α'}\n",
    "\n",
    "Statistical Power:\n",
    "  Sample size: n = {sample_size}\n",
    "  Observed effect size: d = {cohens_d:.3f}\n",
    "  Minimum detectable effect (80% power): d ≈ {mde:.3f}\n",
    "  \n",
    "  Study power assessment: {'✓ Sufficient power' if abs(cohens_d) >= mde else '⚠ May be underpowered'}\n",
    "\n",
    "7. CONCLUSIONS\n",
    "{'-'*80}\n",
    "Based on the Wilcoxon signed rank test with {len(results)} paired observations:\n",
    "\n",
    "• Statistical Significance: {'Significant difference detected (p < α)' if p_value < CONFIG['alpha'] else 'No significant difference detected (p ≥ α)'}\n",
    "• Effect Size: {abs(r):.3f} ({'negligible' if abs(r) < 0.1 else 'small' if abs(r) < 0.3 else 'medium' if abs(r) < 0.5 else 'large'})\n",
    "• Practical Significance: Mean log-prob {'improvement' if df_summary['log_prob_diff'].mean() > 0 else 'decrease'} of {abs(df_summary['log_prob_diff'].mean()):.4f}\n",
    "• Accuracy Impact: {'MCMC shows higher accuracy' if df_summary['mcmc_correct'].mean() > df_summary['std_correct'].mean() else 'Standard shows higher accuracy' if df_summary['std_correct'].mean() > df_summary['mcmc_correct'].mean() else 'Similar accuracy'}\n",
    "\n",
    "Recommendation:\n",
    "{('MCMC sampling shows statistically significant improvement in log-probabilities.' if p_value < CONFIG['alpha'] and df_summary['log_prob_diff'].mean() > 0 else 'Standard sampling shows statistically significant improvement in log-probabilities.' if p_value < CONFIG['alpha'] and df_summary['log_prob_diff'].mean() < 0 else 'No significant difference between methods. Choice may depend on computational cost vs. benefit.')}\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_file = os.path.join(CONFIG['save_dir'], 'wilcoxon_test_report.txt')\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token-level log probabilities\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKEN-LEVEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Aggregate all token log probs\n",
    "all_std_token_logprobs = []\n",
    "all_mcmc_token_logprobs = []\n",
    "\n",
    "for r in results:\n",
    "    all_std_token_logprobs.extend(r['std_log_probs'])\n",
    "    all_mcmc_token_logprobs.extend(r['mcmc_log_probs'])\n",
    "\n",
    "print(f\"\\nTotal tokens analyzed:\")\n",
    "print(f\"  Standard: {len(all_std_token_logprobs)}\")\n",
    "print(f\"  MCMC: {len(all_mcmc_token_logprobs)}\")\n",
    "\n",
    "print(f\"\\nPer-token statistics:\")\n",
    "print(f\"  Standard - Mean: {np.mean(all_std_token_logprobs):.4f}, Std: {np.std(all_std_token_logprobs):.4f}\")\n",
    "print(f\"  MCMC - Mean: {np.mean(all_mcmc_token_logprobs):.4f}, Std: {np.std(all_mcmc_token_logprobs):.4f}\")\n",
    "\n",
    "# Distribution comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograms\n",
    "axes[0].hist(all_std_token_logprobs, bins=50, alpha=0.6, label='Standard', density=True)\n",
    "axes[0].hist(all_mcmc_token_logprobs, bins=50, alpha=0.6, label='MCMC', density=True)\n",
    "axes[0].set_xlabel('Token Log-Probability')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Distribution of Token Log-Probabilities')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# KDE plots\n",
    "from scipy.stats import gaussian_kde\n",
    "kde_std = gaussian_kde(all_std_token_logprobs)\n",
    "kde_mcmc = gaussian_kde(all_mcmc_token_logprobs)\n",
    "x_range = np.linspace(\n",
    "    min(min(all_std_token_logprobs), min(all_mcmc_token_logprobs)),\n",
    "    max(max(all_std_token_logprobs), max(all_mcmc_token_logprobs)),\n",
    "    1000\n",
    ")\n",
    "axes[1].plot(x_range, kde_std(x_range), label='Standard', linewidth=2)\n",
    "axes[1].plot(x_range, kde_mcmc(x_range), label='MCMC', linewidth=2)\n",
    "axes[1].fill_between(x_range, kde_std(x_range), alpha=0.3)\n",
    "axes[1].fill_between(x_range, kde_mcmc(x_range), alpha=0.3)\n",
    "axes[1].set_xlabel('Token Log-Probability')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Kernel Density Estimate')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'token_level_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Data for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    'config': CONFIG,\n",
    "    'sample_size': len(results),\n",
    "    'wilcoxon_test': {\n",
    "        'two_tailed': {\n",
    "            'statistic': float(statistic),\n",
    "            'p_value': float(p_value),\n",
    "            'effect_size_r': float(r),\n",
    "        },\n",
    "        'one_tailed_greater': {\n",
    "            'statistic': float(statistic_gt),\n",
    "            'p_value': float(p_value_gt),\n",
    "        },\n",
    "    },\n",
    "    'paired_ttest': {\n",
    "        't_statistic': float(t_stat),\n",
    "        'p_value': float(t_pvalue),\n",
    "        'cohens_d': float(cohens_d),\n",
    "    },\n",
    "    'normality_test': {\n",
    "        'shapiro_statistic': float(stat_shapiro),\n",
    "        'shapiro_p_value': float(p_shapiro),\n",
    "    },\n",
    "    'power_analysis': {\n",
    "        'type_i_error': float(type_i_error),\n",
    "        'minimum_detectable_effect': float(mde),\n",
    "    },\n",
    "    'descriptive_stats': {\n",
    "        'standard': {\n",
    "            'mean_cumulative_logprob': float(df_summary['std_cumulative_log_prob'].mean()),\n",
    "            'std_cumulative_logprob': float(df_summary['std_cumulative_log_prob'].std()),\n",
    "            'mean_tokens': float(df_summary['std_num_tokens'].mean()),\n",
    "            'accuracy': float(df_summary['std_correct'].mean()),\n",
    "        },\n",
    "        'mcmc': {\n",
    "            'mean_cumulative_logprob': float(df_summary['mcmc_cumulative_log_prob'].mean()),\n",
    "            'std_cumulative_logprob': float(df_summary['mcmc_cumulative_log_prob'].std()),\n",
    "            'mean_tokens': float(df_summary['mcmc_num_tokens'].mean()),\n",
    "            'accuracy': float(df_summary['mcmc_correct'].mean()),\n",
    "            'mean_acceptance_ratio': float(df_summary['mcmc_acceptance_ratio'].mean()),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "stats_file = os.path.join(CONFIG['save_dir'], 'summary_statistics.json')\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"Summary statistics saved to: {stats_file}\")\n",
    "\n",
    "# Export DataFrame\n",
    "df_file = os.path.join(CONFIG['save_dir'], 'analysis_dataframe.csv')\n",
    "df_summary.to_csv(df_file, index=False)\n",
    "print(f\"Analysis dataframe saved to: {df_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll results saved to: {CONFIG['save_dir']}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for filename in os.listdir(CONFIG['save_dir']):\n",
    "    filepath = os.path.join(CONFIG['save_dir'], filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {filename} ({size:,} bytes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
