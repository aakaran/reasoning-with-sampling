{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH 500 Dataset Preview and Quick Test\n",
    "\n",
    "This notebook demonstrates how the Soft Thinking notebooks use the MATH 500 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "\n",
    "# Load MATH 500 dataset\n",
    "dataset_path = '/home/wliu23/github/reasoning-with-sampling/llm_experiments/data/MATH500.json'\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(dataset)} problems from MATH 500 dataset\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"  Fields: {list(dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for i, q in enumerate(dataset):\n",
    "    summary_data.append({\n",
    "        'ID': i,\n",
    "        'Source': q.get('source', 'Unknown'),\n",
    "        'Question Preview': q['prompt'][:80] + '...',\n",
    "        'Answer Preview': str(q['answer'])[:50] + ('...' if len(str(q['answer'])) > 50 else ''),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nFirst 10 questions:\")\n",
    "display(df_summary.head(10))\n",
    "\n",
    "# Show source distribution\n",
    "print(\"\\nProblem sources:\")\n",
    "print(df_summary['Source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Questions with Rendered Math\n",
    "\n",
    "Here are some example questions with properly rendered LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_math(text):\n",
    "    \"\"\"Render text with proper LaTeX formatting.\"\"\"\n",
    "    # Convert LaTeX delimiters\n",
    "    text = re.sub(r'\\\\\\[', '$$', text)\n",
    "    text = re.sub(r'\\\\\\]', '$$', text)\n",
    "    text = re.sub(r'\\\\\\(', '$', text)\n",
    "    text = re.sub(r'\\\\\\)', '$', text)\n",
    "    return Markdown(text)\n",
    "\n",
    "# Display first 5 questions with math rendering\n",
    "for i in range(5):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i+1}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    display(render_math(f\"**Problem:** {dataset[i]['prompt']}\"))\n",
    "    display(render_math(f\"**Answer:** {dataset[i]['answer']}\"))\n",
    "    print(f\"Source: {dataset[i].get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Comparison Notebooks Use This Data\n",
    "\n",
    "Both comparison notebooks (`standard_vs_soft_thinking.ipynb` and `comprehensive_soft_thinking_comparison.ipynb`) process MATH 500 questions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how notebooks process each question\n",
    "print(\"Processing workflow for each question:\\n\")\n",
    "\n",
    "example_question = dataset[0]\n",
    "\n",
    "print(\"1. Extract question and answer:\")\n",
    "print(f\"   Question: {example_question['prompt'][:100]}...\")\n",
    "print(f\"   Correct Answer: {example_question['answer']}\")\n",
    "\n",
    "print(\"\\n2. Generate with different methods:\")\n",
    "print(\"   - Standard Sampling â†’ Get answer, tokens, log-probs\")\n",
    "print(\"   - Vanilla Soft Thinking â†’ Get answer, tokens, log-probs, thinking steps\")\n",
    "print(\"   - Dirichlet Soft Thinking â†’ With noise injection\")\n",
    "print(\"   - Gumbel Soft Thinking â†’ With Gumbel-Softmax noise\")\n",
    "\n",
    "print(\"\\n3. Compare results:\")\n",
    "print(\"   - Check correctness (answer == correct_answer)\")\n",
    "print(\"   - Measure efficiency (number of tokens)\")\n",
    "print(\"   - Analyze quality (log-probabilities)\")\n",
    "print(\"   - Track thinking process (steps, entropy, stopping reasons)\")\n",
    "\n",
    "print(\"\\n4. Statistical analysis:\")\n",
    "print(\"   - Wilcoxon test (pairwise comparisons)\")\n",
    "print(\"   - Friedman test (multiple methods)\")\n",
    "print(\"   - Effect size calculations\")\n",
    "print(\"   - Visualization dashboards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for Full Dataset vs Subset\n",
    "\n",
    "The notebooks are configured to run on a **subset** by default for quick testing, but can easily use the **full 500 questions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default configuration (for testing)\n",
    "CONFIG_TEST = {\n",
    "    'num_questions': 20,  # Small subset for quick testing\n",
    "    'dataset_path': '/home/wliu23/github/reasoning-with-sampling/llm_experiments/data/MATH500.json',\n",
    "}\n",
    "\n",
    "# Full dataset configuration (for complete analysis)\n",
    "CONFIG_FULL = {\n",
    "    'num_questions': 500,  # Use all questions\n",
    "    'dataset_path': '/home/wliu23/github/reasoning-with-sampling/llm_experiments/data/MATH500.json',\n",
    "}\n",
    "\n",
    "print(\"Test configuration (default):\")\n",
    "print(f\"  Processing {CONFIG_TEST['num_questions']} questions\")\n",
    "print(f\"  Estimated time: ~5-10 minutes per method\")\n",
    "print(f\"  Best for: Quick validation and testing\")\n",
    "\n",
    "print(\"\\nFull configuration:\")\n",
    "print(f\"  Processing {CONFIG_FULL['num_questions']} questions\")\n",
    "print(f\"  Estimated time: ~2-4 hours per method (with 8 GPUs in parallel)\")\n",
    "print(f\"  Best for: Complete statistical analysis and publication results\")\n",
    "\n",
    "print(\"\\nðŸ’¡ To use full dataset, simply change:\")\n",
    "print(\"   CONFIG['num_questions'] = 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Question Types in MATH 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize questions by complexity indicators\n",
    "def estimate_complexity(question):\n",
    "    \"\"\"Rough complexity estimate based on question length and LaTeX usage.\"\"\"\n",
    "    has_latex = '\\\\' in question['prompt'] or '$' in question['prompt']\n",
    "    length = len(question['prompt'])\n",
    "    \n",
    "    if length > 500:\n",
    "        return 'Complex' if has_latex else 'Long'\n",
    "    elif length > 200:\n",
    "        return 'Medium' if has_latex else 'Standard'\n",
    "    else:\n",
    "        return 'Short'\n",
    "\n",
    "complexity_counts = {}\n",
    "for q in dataset:\n",
    "    complexity = estimate_complexity(q)\n",
    "    complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1\n",
    "\n",
    "print(\"Question complexity distribution:\")\n",
    "for complexity, count in sorted(complexity_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {complexity}: {count} questions ({count/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "# Show examples of each type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example questions by complexity:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shown_types = set()\n",
    "for q in dataset:\n",
    "    complexity = estimate_complexity(q)\n",
    "    if complexity not in shown_types:\n",
    "        shown_types.add(complexity)\n",
    "        print(f\"\\n[{complexity}]\")\n",
    "        print(f\"Q: {q['prompt'][:150]}...\")\n",
    "        print(f\"A: {q['answer'][:80]}...\" if len(str(q['answer'])) > 80 else f\"A: {q['answer']}\")\n",
    "        if len(shown_types) >= 3:  # Show 3 examples\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Ready to Run!\n",
    "\n",
    "The MATH 500 dataset is properly loaded and ready for use in:\n",
    "\n",
    "1. **`standard_vs_soft_thinking.ipynb`** - Basic comparison (Standard vs Soft Thinking)\n",
    "2. **`comprehensive_soft_thinking_comparison.ipynb`** - Full comparison (4 methods with noise variants)\n",
    "\n",
    "Both notebooks will:\n",
    "- Load these 500 mathematical problems\n",
    "- Process them with different sampling methods\n",
    "- Compare accuracy, efficiency, and reasoning quality\n",
    "- Generate comprehensive statistical analysis and visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
