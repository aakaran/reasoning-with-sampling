{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Sampling vs Soft Thinking: Comparative Analysis on MATH 500\n",
    "\n",
    "This notebook compares two generation strategies for mathematical reasoning:\n",
    "\n",
    "1. **Standard Sampling**: Traditional discrete token-by-token generation\n",
    "2. **Soft Thinking**: Generation in continuous concept space using soft token embeddings\n",
    "\n",
    "## What is Soft Thinking?\n",
    "\n",
    "Soft Thinking is a novel approach that enables LLMs to reason in a **continuous concept space** rather than discrete tokens:\n",
    "\n",
    "- **Traditional approach**: Model commits to a discrete token at each step\n",
    "- **Soft Thinking**: Model creates \"soft tokens\" as probability-weighted mixtures:\n",
    "  $$h_t = \\sum_{v} p(v|\\text{context}) \\cdot \\text{embedding}(v)$$\n",
    "\n",
    "This allows the model to explore multiple candidate tokens simultaneously before committing to a final choice.\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Richer representations**: Operates in continuous space, transcending discrete boundaries\n",
    "- **Flexible reasoning**: Can \"think\" through multiple possibilities\n",
    "- **Early stopping**: Uses entropy-based thresholds to commit when confident\n",
    "- **Training-free**: No additional training required\n",
    "\n",
    "## Analysis Plan\n",
    "\n",
    "1. **Data Collection**: Run both methods on MATH 500 questions\n",
    "2. **Metrics Comparison**: Accuracy, log-probabilities, generation efficiency\n",
    "3. **Statistical Analysis**: Test for significant differences\n",
    "4. **Thinking Process Analysis**: Examine soft thinking steps and entropy patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, mannwhitneyu, ttest_rel\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# Add project path\n",
    "project_root = Path('/home/wliu23/github/reasoning-with-sampling/llm_experiments')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from grader_utils.parse_utils import parse_answer\n",
    "from constants import *\n",
    "from power_samp_utils import AutoregressiveSampler, SoftThinkingSampler, soft_thinking_generate, format_prompt\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up HuggingFace cache\n",
    "HF_HOME = Path(\"/home/wliu23/projects/reasoning-with-sampling/.hf_home\")\n",
    "os.environ[\"HF_HOME\"] = str(HF_HOME)\n",
    "os.environ[\"HF_HUB_CACHE\"] = str(HF_HOME / \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_HOME / \"hub\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(HF_HOME / \"datasets\")\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model': 'qwen_math',\n",
    "    'model_str': 'Qwen/Qwen2.5-Math-7B',\n",
    "    'cache_dir': os.environ['TRANSFORMERS_CACHE'],\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Dataset settings\n",
    "    'dataset': 'MATH',\n",
    "    'dataset_path': '/home/wliu23/github/reasoning-with-sampling/llm_experiments/data/MATH500.json',\n",
    "    'num_questions': 20,  # Start small for testing\n",
    "    \n",
    "    # Generation settings\n",
    "    'max_new_tokens': 3072,\n",
    "    'cot': True,\n",
    "    \n",
    "    # Soft Thinking hyperparameters\n",
    "    'soft_thinking': {\n",
    "        'num_thinking_steps': 3,  # Number of soft thinking steps per token\n",
    "        'max_topk': 10,  # Top-k tokens for soft embedding\n",
    "        'min_p': 0.001,  # Minimum probability threshold\n",
    "        'early_stopping_entropy_threshold': 0.05,  # Stop thinking if entropy < this\n",
    "        'temperature': 1.0,\n",
    "    },\n",
    "    \n",
    "    # Standard sampling settings\n",
    "    'standard': {\n",
    "        'temperature': 1.0,\n",
    "    },\n",
    "    \n",
    "    # Output settings\n",
    "    'save_dir': '/home/wliu23/github/reasoning-with-sampling/notebooks/results/soft_thinking',\n",
    "    'alpha': 0.05,  # Statistical significance level\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {CONFIG['model_str']}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "print(f\"  Questions: {CONFIG['num_questions']}\")\n",
    "print(f\"  Soft Thinking Steps: {CONFIG['soft_thinking']['num_thinking_steps']}\")\n",
    "print(f\"  Max Top-K: {CONFIG['soft_thinking']['max_topk']}\")\n",
    "print(f\"  Results: {CONFIG['save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(CONFIG['dataset_path'], 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} problems from MATH500\")\n",
    "print(f\"Will process {CONFIG['num_questions']} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {CONFIG['model_str']}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model_str'],\n",
    "    cache_dir=CONFIG['cache_dir'],\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set pad_token_id\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_str'],\n",
    "    cache_dir=CONFIG['cache_dir'],\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={'': CONFIG['device']},\n",
    "    trust_remote_code=True,\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "print(f\"Model loaded on {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize samplers\n",
    "autoreg_sampler = AutoregressiveSampler(model, tokenizer, CONFIG['device'])\n",
    "\n",
    "soft_thinking_sampler = SoftThinkingSampler(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    CONFIG['device'],\n",
    "    max_topk=CONFIG['soft_thinking']['max_topk'],\n",
    "    min_p=CONFIG['soft_thinking']['min_p'],\n",
    "    early_stopping_entropy_threshold=CONFIG['soft_thinking']['early_stopping_entropy_threshold'],\n",
    "    temperature=CONFIG['soft_thinking']['temperature']\n",
    ")\n",
    "\n",
    "print(\"Samplers initialized\")\n",
    "print(f\"  Standard: AutoregressiveSampler\")\n",
    "print(f\"  Soft Thinking: max_topk={CONFIG['soft_thinking']['max_topk']}, \"\n",
    "      f\"num_steps={CONFIG['soft_thinking']['num_thinking_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_standard(model, tokenizer, input_ids, device, max_new_tokens=3072, temperature=1.0) -> Dict:\n",
    "    \"\"\"Generate using standard sampling.\"\"\"\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    # Extract generated tokens\n",
    "    generated_ids = output.sequences[0][len(input_ids[0]):]\n",
    "    completion = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    \n",
    "    # Get log probabilities\n",
    "    log_probs = []\n",
    "    tokens = []\n",
    "    \n",
    "    for i, token_id in enumerate(generated_ids):\n",
    "        if i < len(output.scores):\n",
    "            logits = output.scores[i][0]\n",
    "            log_prob_dist = F.log_softmax(logits, dim=-1)\n",
    "            token_log_prob = log_prob_dist[token_id].item()\n",
    "            log_probs.append(token_log_prob)\n",
    "        tokens.append(tokenizer.decode([token_id]))\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids.cpu().tolist(),\n",
    "        'log_probs': log_probs,\n",
    "        'cumulative_log_prob': sum(log_probs) if log_probs else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs) if log_probs else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_soft_thinking(soft_sampler, prefix, max_new_tokens=3072, num_thinking_steps=3) -> Dict:\n",
    "    \"\"\"Generate using Soft Thinking.\"\"\"\n",
    "    token_ids, log_probs, soft_info, avg_thinking_steps = soft_thinking_generate(\n",
    "        soft_sampler,\n",
    "        prefix,\n",
    "        max_new_tokens,\n",
    "        num_thinking_steps\n",
    "    )\n",
    "    \n",
    "    # Remove prefix to get only generated tokens\n",
    "    generated_ids = token_ids[len(prefix):]\n",
    "    \n",
    "    # Decode\n",
    "    completion = soft_sampler.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    parsed_answer = parse_answer(completion)\n",
    "    \n",
    "    # Decode individual tokens\n",
    "    tokens = [soft_sampler.tokenizer.decode([tid]) for tid in generated_ids]\n",
    "    \n",
    "    return {\n",
    "        'completion': completion,\n",
    "        'answer': parsed_answer,\n",
    "        'tokens': tokens,\n",
    "        'token_ids': generated_ids,\n",
    "        'log_probs': log_probs,\n",
    "        'cumulative_log_prob': sum(log_probs) if log_probs else 0.0,\n",
    "        'num_tokens': len(tokens),\n",
    "        'mean_log_prob': np.mean(log_probs) if log_probs else 0.0,\n",
    "        'avg_thinking_steps': avg_thinking_steps,\n",
    "        'soft_info': soft_info,\n",
    "        'total_thinking_steps': sum(info['thinking_steps'] for info in soft_info),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection\n",
    "results = []\n",
    "questions_to_process = dataset[:CONFIG['num_questions']]\n",
    "\n",
    "for idx, data in enumerate(tqdm(questions_to_process, desc=\"Processing MATH problems\")):\n",
    "    question = data['prompt']\n",
    "    correct_answer = data['answer']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {idx+1}/{len(questions_to_process)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Q: {question[:100]}...\")\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = format_prompt(question, CONFIG['model'], tokenizer, CONFIG['cot'])\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(CONFIG['device'])\n",
    "    prefix = [idx.item() for idx in input_ids[0]]\n",
    "    \n",
    "    # Standard sampling\n",
    "    print(\"\\n[Standard Sampling]\")\n",
    "    std_result = generate_standard(\n",
    "        model, tokenizer, input_ids, CONFIG['device'],\n",
    "        CONFIG['max_new_tokens'], CONFIG['standard']['temperature']\n",
    "    )\n",
    "    print(f\"  Answer: {std_result['answer']}\")\n",
    "    print(f\"  Tokens: {std_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {std_result['cumulative_log_prob']:.4f}\")\n",
    "    \n",
    "    # Soft Thinking\n",
    "    print(\"\\n[Soft Thinking]\")\n",
    "    soft_result = generate_soft_thinking(\n",
    "        soft_thinking_sampler, prefix,\n",
    "        CONFIG['max_new_tokens'],\n",
    "        CONFIG['soft_thinking']['num_thinking_steps']\n",
    "    )\n",
    "    print(f\"  Answer: {soft_result['answer']}\")\n",
    "    print(f\"  Tokens: {soft_result['num_tokens']}\")\n",
    "    print(f\"  Cumulative log-prob: {soft_result['cumulative_log_prob']:.4f}\")\n",
    "    print(f\"  Avg thinking steps: {soft_result['avg_thinking_steps']:.2f}\")\n",
    "    print(f\"  Total thinking steps: {soft_result['total_thinking_steps']}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'question_idx': idx,\n",
    "        'question': question,\n",
    "        'correct_answer': correct_answer,\n",
    "        \n",
    "        # Standard sampling\n",
    "        'std_completion': std_result['completion'],\n",
    "        'std_answer': std_result['answer'],\n",
    "        'std_tokens': std_result['tokens'],\n",
    "        'std_token_ids': std_result['token_ids'],\n",
    "        'std_log_probs': std_result['log_probs'],\n",
    "        'std_cumulative_log_prob': std_result['cumulative_log_prob'],\n",
    "        'std_num_tokens': std_result['num_tokens'],\n",
    "        'std_mean_log_prob': std_result['mean_log_prob'],\n",
    "        'std_correct': std_result['answer'] == correct_answer,\n",
    "        \n",
    "        # Soft Thinking\n",
    "        'soft_completion': soft_result['completion'],\n",
    "        'soft_answer': soft_result['answer'],\n",
    "        'soft_tokens': soft_result['tokens'],\n",
    "        'soft_token_ids': soft_result['token_ids'],\n",
    "        'soft_log_probs': soft_result['log_probs'],\n",
    "        'soft_cumulative_log_prob': soft_result['cumulative_log_prob'],\n",
    "        'soft_num_tokens': soft_result['num_tokens'],\n",
    "        'soft_mean_log_prob': soft_result['mean_log_prob'],\n",
    "        'soft_avg_thinking_steps': soft_result['avg_thinking_steps'],\n",
    "        'soft_total_thinking_steps': soft_result['total_thinking_steps'],\n",
    "        'soft_correct': soft_result['answer'] == correct_answer,\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\nCompleted data collection for {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_file = os.path.join(CONFIG['save_dir'], 'soft_thinking_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        'question_idx': r['question_idx'],\n",
    "        'std_cumulative_log_prob': r['std_cumulative_log_prob'],\n",
    "        'soft_cumulative_log_prob': r['soft_cumulative_log_prob'],\n",
    "        'std_mean_log_prob': r['std_mean_log_prob'],\n",
    "        'soft_mean_log_prob': r['soft_mean_log_prob'],\n",
    "        'std_num_tokens': r['std_num_tokens'],\n",
    "        'soft_num_tokens': r['soft_num_tokens'],\n",
    "        'std_correct': r['std_correct'],\n",
    "        'soft_correct': r['soft_correct'],\n",
    "        'soft_avg_thinking_steps': r['soft_avg_thinking_steps'],\n",
    "        'soft_total_thinking_steps': r['soft_total_thinking_steps'],\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStandard Sampling:\")\n",
    "print(f\"  Mean cumulative log-prob: {df_summary['std_cumulative_log_prob'].mean():.4f}\")\n",
    "print(f\"  Std cumulative log-prob: {df_summary['std_cumulative_log_prob'].std():.4f}\")\n",
    "print(f\"  Mean tokens: {df_summary['std_num_tokens'].mean():.2f}\")\n",
    "print(f\"  Accuracy: {df_summary['std_correct'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nSoft Thinking:\")\n",
    "print(f\"  Mean cumulative log-prob: {df_summary['soft_cumulative_log_prob'].mean():.4f}\")\n",
    "print(f\"  Std cumulative log-prob: {df_summary['soft_cumulative_log_prob'].std():.4f}\")\n",
    "print(f\"  Mean tokens: {df_summary['soft_num_tokens'].mean():.2f}\")\n",
    "print(f\"  Accuracy: {df_summary['soft_correct'].mean():.2%}\")\n",
    "print(f\"  Avg thinking steps per token: {df_summary['soft_avg_thinking_steps'].mean():.2f}\")\n",
    "print(f\"  Avg total thinking steps: {df_summary['soft_total_thinking_steps'].mean():.2f}\")\n",
    "\n",
    "# Calculate differences\n",
    "df_summary['log_prob_diff'] = df_summary['soft_cumulative_log_prob'] - df_summary['std_cumulative_log_prob']\n",
    "df_summary['token_diff'] = df_summary['soft_num_tokens'] - df_summary['std_num_tokens']\n",
    "\n",
    "print(\"\\nDifferences (Soft Thinking - Standard):\")\n",
    "print(f\"  Mean log-prob difference: {df_summary['log_prob_diff'].mean():.4f}\")\n",
    "print(f\"  Mean token difference: {df_summary['token_diff'].mean():.2f}\")\n",
    "print(f\"  Accuracy improvement: {(df_summary['soft_correct'].mean() - df_summary['std_correct'].mean()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Log-prob comparison\n",
    "axes[0, 0].scatter(df_summary['std_cumulative_log_prob'],\n",
    "                   df_summary['soft_cumulative_log_prob'], alpha=0.6)\n",
    "lims = [min(df_summary['std_cumulative_log_prob'].min(), df_summary['soft_cumulative_log_prob'].min()),\n",
    "        max(df_summary['std_cumulative_log_prob'].max(), df_summary['soft_cumulative_log_prob'].max())]\n",
    "axes[0, 0].plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "axes[0, 0].set_xlabel('Standard Cumulative Log-Prob')\n",
    "axes[0, 0].set_ylabel('Soft Thinking Cumulative Log-Prob')\n",
    "axes[0, 0].set_title('Cumulative Log-Prob Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution of differences\n",
    "axes[0, 1].hist(df_summary['log_prob_diff'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(0, color='r', linestyle='--', label='Zero difference')\n",
    "axes[0, 1].axvline(df_summary['log_prob_diff'].mean(), color='g',\n",
    "                   linestyle='--', label=f'Mean = {df_summary[\"log_prob_diff\"].mean():.2f}')\n",
    "axes[0, 1].set_xlabel('Log-Prob Difference (Soft - Standard)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Differences')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Box plot comparison\n",
    "box_data = [df_summary['std_cumulative_log_prob'], df_summary['soft_cumulative_log_prob']]\n",
    "axes[0, 2].boxplot(box_data, labels=['Standard', 'Soft Thinking'])\n",
    "axes[0, 2].set_ylabel('Cumulative Log-Prob')\n",
    "axes[0, 2].set_title('Distribution Comparison')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Token count comparison\n",
    "axes[1, 0].scatter(df_summary['std_num_tokens'],\n",
    "                   df_summary['soft_num_tokens'], alpha=0.6)\n",
    "lims = [min(df_summary['std_num_tokens'].min(), df_summary['soft_num_tokens'].min()),\n",
    "        max(df_summary['std_num_tokens'].max(), df_summary['soft_num_tokens'].max())]\n",
    "axes[1, 0].plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "axes[1, 0].set_xlabel('Standard Token Count')\n",
    "axes[1, 0].set_ylabel('Soft Thinking Token Count')\n",
    "axes[1, 0].set_title('Token Count Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Thinking steps distribution\n",
    "axes[1, 1].hist(df_summary['soft_avg_thinking_steps'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(df_summary['soft_avg_thinking_steps'].mean(), color='r',\n",
    "                   linestyle='--', label=f'Mean = {df_summary[\"soft_avg_thinking_steps\"].mean():.2f}')\n",
    "axes[1, 1].set_xlabel('Average Thinking Steps per Token')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Soft Thinking Steps Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Accuracy comparison\n",
    "accuracy_data = [\n",
    "    ['Standard', df_summary['std_correct'].sum(), len(df_summary) - df_summary['std_correct'].sum()],\n",
    "    ['Soft Thinking', df_summary['soft_correct'].sum(), len(df_summary) - df_summary['soft_correct'].sum()]\n",
    "]\n",
    "methods = [x[0] for x in accuracy_data]\n",
    "correct = [x[1] for x in accuracy_data]\n",
    "incorrect = [x[2] for x in accuracy_data]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "axes[1, 2].bar(x, correct, width, label='Correct', alpha=0.8)\n",
    "axes[1, 2].bar(x, incorrect, width, bottom=correct, label='Incorrect', alpha=0.8)\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].set_title('Accuracy Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(methods)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['save_dir'], 'comparison_plots.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon signed-rank test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test for log-probabilities\n",
    "statistic, p_value = wilcoxon(\n",
    "    df_summary['soft_cumulative_log_prob'],\n",
    "    df_summary['std_cumulative_log_prob'],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "print(\"\\nWilcoxon Signed-Rank Test (Log-Probabilities):\")\n",
    "print(f\"  H₀: median(Soft - Standard) = 0\")\n",
    "print(f\"  H₁: median(Soft - Standard) ≠ 0\")\n",
    "print(f\"  Test statistic: {statistic:.4f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "print(f\"  Significance level: {CONFIG['alpha']}\")\n",
    "\n",
    "if p_value < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀: Significant difference (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀: No significant difference (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, t_pvalue = ttest_rel(\n",
    "    df_summary['soft_cumulative_log_prob'],\n",
    "    df_summary['std_cumulative_log_prob']\n",
    ")\n",
    "\n",
    "print(\"\\nPaired t-test (Log-Probabilities):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {t_pvalue:.6f}\")\n",
    "\n",
    "if t_pvalue < CONFIG['alpha']:\n",
    "    print(f\"  ✓ REJECT H₀ (p < {CONFIG['alpha']})\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL TO REJECT H₀ (p >= {CONFIG['alpha']})\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "cohens_d = df_summary['log_prob_diff'].mean() / df_summary['log_prob_diff'].std()\n",
    "print(f\"\\nCohen's d: {cohens_d:.4f}\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    print(\"  Interpretation: Small effect\")\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    print(\"  Interpretation: Medium effect\")\n",
    "else:\n",
    "    print(\"  Interpretation: Large effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math rendering helper\n",
    "def render_math_question(question_text, max_length=None):\n",
    "    \"\"\"Render question with proper LaTeX.\"\"\"\n",
    "    if max_length and len(question_text) > max_length:\n",
    "        display_text = question_text[:max_length] + \"...\"\n",
    "    else:\n",
    "        display_text = question_text\n",
    "    \n",
    "    # Convert LaTeX delimiters\n",
    "    display_text = re.sub(r'\\\\\\[', '$$', display_text)\n",
    "    display_text = re.sub(r'\\\\\\]', '$$', display_text)\n",
    "    display_text = re.sub(r'\\\\\\(', '$', display_text)\n",
    "    display_text = re.sub(r'\\\\\\)', '$', display_text)\n",
    "    \n",
    "    return Markdown(display_text)\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED PER-QUESTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in results[:5]:  # Show first 5\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {result['question_idx'] + 1}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    display(render_math_question(result['question']))\n",
    "    \n",
    "    # Comparison table\n",
    "    comparison = pd.DataFrame([\n",
    "        {\n",
    "            'Method': 'Standard',\n",
    "            'Answer': result['std_answer'],\n",
    "            'Correct': '✓' if result['std_correct'] else '✗',\n",
    "            'Tokens': result['std_num_tokens'],\n",
    "            'Cumulative Log-Prob': f\"{result['std_cumulative_log_prob']:.4f}\",\n",
    "            'Mean Log-Prob': f\"{result['std_mean_log_prob']:.4f}\",\n",
    "        },\n",
    "        {\n",
    "            'Method': 'Soft Thinking',\n",
    "            'Answer': result['soft_answer'],\n",
    "            'Correct': '✓' if result['soft_correct'] else '✗',\n",
    "            'Tokens': result['soft_num_tokens'],\n",
    "            'Cumulative Log-Prob': f\"{result['soft_cumulative_log_prob']:.4f}\",\n",
    "            'Mean Log-Prob': f\"{result['soft_mean_log_prob']:.4f}\",\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    display(HTML(comparison.to_html(index=False)))\n",
    "    \n",
    "    print(f\"\\nCorrect Answer: {result['correct_answer']}\")\n",
    "    print(f\"Soft Thinking - Avg Steps/Token: {result['soft_avg_thinking_steps']:.2f}\")\n",
    "    print(f\"Soft Thinking - Total Steps: {result['soft_total_thinking_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "STANDARD SAMPLING VS SOFT THINKING: COMPREHENSIVE ANALYSIS\n",
    "{'='*80}\n",
    "\n",
    "1. CONFIGURATION\n",
    "{'-'*80}\n",
    "Model: {CONFIG['model_str']}\n",
    "Dataset: MATH500\n",
    "Questions: {len(results)}\n",
    "Soft Thinking Steps: {CONFIG['soft_thinking']['num_thinking_steps']}\n",
    "Max Top-K: {CONFIG['soft_thinking']['max_topk']}\n",
    "Early Stopping Entropy: {CONFIG['soft_thinking']['early_stopping_entropy_threshold']}\n",
    "\n",
    "2. PERFORMANCE METRICS\n",
    "{'-'*80}\n",
    "Standard Sampling:\n",
    "  Accuracy: {df_summary['std_correct'].mean():.2%} ({df_summary['std_correct'].sum()}/{len(df_summary)})\n",
    "  Mean cumulative log-prob: {df_summary['std_cumulative_log_prob'].mean():.4f}\n",
    "  Mean tokens: {df_summary['std_num_tokens'].mean():.2f}\n",
    "\n",
    "Soft Thinking:\n",
    "  Accuracy: {df_summary['soft_correct'].mean():.2%} ({df_summary['soft_correct'].sum()}/{len(df_summary)})\n",
    "  Mean cumulative log-prob: {df_summary['soft_cumulative_log_prob'].mean():.4f}\n",
    "  Mean tokens: {df_summary['soft_num_tokens'].mean():.2f}\n",
    "  Avg thinking steps/token: {df_summary['soft_avg_thinking_steps'].mean():.2f}\n",
    "\n",
    "Improvement:\n",
    "  Accuracy: {(df_summary['soft_correct'].mean() - df_summary['std_correct'].mean()):.2%}\n",
    "  Log-prob: {df_summary['log_prob_diff'].mean():.4f}\n",
    "  Token efficiency: {df_summary['token_diff'].mean():.2f} tokens\n",
    "\n",
    "3. STATISTICAL SIGNIFICANCE\n",
    "{'-'*80}\n",
    "Wilcoxon Test:\n",
    "  Statistic: {statistic:.4f}\n",
    "  p-value: {p_value:.6f}\n",
    "  Result: {'SIGNIFICANT' if p_value < CONFIG['alpha'] else 'NOT SIGNIFICANT'}\n",
    "\n",
    "Paired t-test:\n",
    "  t-statistic: {t_stat:.4f}\n",
    "  p-value: {t_pvalue:.6f}\n",
    "\n",
    "Effect Size (Cohen's d): {cohens_d:.4f}\n",
    "\n",
    "4. KEY FINDINGS\n",
    "{'-'*80}\n",
    "• Soft Thinking {'improves' if df_summary['soft_correct'].mean() > df_summary['std_correct'].mean() else 'does not improve'} accuracy\n",
    "• Log-probabilities are {'higher' if df_summary['log_prob_diff'].mean() > 0 else 'lower'} with Soft Thinking\n",
    "• Token count is {'reduced' if df_summary['token_diff'].mean() < 0 else 'increased'} by {abs(df_summary['token_diff'].mean()):.2f} on average\n",
    "• Average {df_summary['soft_avg_thinking_steps'].mean():.2f} thinking steps per token\n",
    "\n",
    "5. CONCLUSION\n",
    "{'-'*80}\n",
    "Soft Thinking demonstrates {'statistically significant' if p_value < CONFIG['alpha'] else 'no significant'} difference\n",
    "compared to standard sampling on MATH500 dataset. The method uses continuous concept\n",
    "space reasoning with an average of {df_summary['soft_avg_thinking_steps'].mean():.2f} thinking steps per token.\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_file = os.path.join(CONFIG['save_dir'], 'comparison_report.txt')\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary DataFrame\n",
    "df_file = os.path.join(CONFIG['save_dir'], 'comparison_summary.csv')\n",
    "df_summary.to_csv(df_file, index=False)\n",
    "print(f\"Summary dataframe saved to: {df_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll results saved to: {CONFIG['save_dir']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
